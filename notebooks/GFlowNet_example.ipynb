{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Winkielek/gflownet/blob/reproduce/notebooks/GFlowNet_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf gflownet\n",
        "!git clone https://github.com/Winkielek/gflownet.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FywnVzx73D5P",
        "outputId": "7a73d9dc-94de-4f07-899a-ba74728e45b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gflownet'...\n",
            "remote: Enumerating objects: 1338, done.\u001b[K\n",
            "remote: Counting objects: 100% (1338/1338), done.\u001b[K\n",
            "remote: Compressing objects: 100% (607/607), done.\u001b[K\n",
            "remote: Total 1338 (delta 740), reused 1074 (delta 592), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1338/1338), 4.60 MiB | 26.49 MiB/s, done.\n",
            "Resolving deltas: 100% (740/740), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
        "!pip install -e ./gflownet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dm736Vj6tqMj",
        "outputId": "2fe8782b-50d6-4bdf-c39c-6ba9740ef7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_sparse-0.6.18%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.3)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n",
            "Installing collected packages: torch-scatter, torch-sparse\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu118 torch-sparse-0.6.18+pt21cu118\n",
            "Obtaining file:///content/gflownet\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rdkit (from gflownet==0.1.0)\n",
            "  Downloading rdkit-2023.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tables in /usr/local/lib/python3.10/dist-packages (from gflownet==0.1.0) (3.8.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gflownet==0.1.0) (1.11.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from gflownet==0.1.0) (3.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from gflownet==0.1.0) (2.14.1)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.10/dist-packages (from gflownet==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from gflownet==0.1.0) (9.0.0)\n",
            "Collecting gitpython (from gflownet==0.1.0)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botorch (from gflownet==0.1.0)\n",
            "  Downloading botorch-0.9.3-py3-none-any.whl (582 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.3/582.3 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyro-ppl (from gflownet==0.1.0)\n",
            "  Downloading pyro_ppl-1.8.6-py3-none-any.whl (732 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.8/732.8 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpytorch (from gflownet==0.1.0)\n",
            "  Downloading gpytorch-1.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf>=2.3 (from gflownet==0.1.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.3->gflownet==0.1.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.3->gflownet==0.1.0) (6.0.1)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from botorch->gflownet==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from botorch->gflownet==0.1.0) (2.1.0+cu118)\n",
            "Collecting linear-operator==0.5.1 (from botorch->gflownet==0.1.0)\n",
            "  Downloading linear_operator-0.5.1-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch->gflownet==0.1.0) (1.2.2)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator==0.5.1->botorch->gflownet==0.1.0)\n",
            "  Downloading jaxtyping-0.2.23-py3-none-any.whl (29 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator==0.5.1->botorch->gflownet==0.1.0)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl->gflownet==0.1.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl->gflownet==0.1.0) (3.3.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl->gflownet==0.1.0)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl->gflownet==0.1.0) (4.66.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->gflownet==0.1.0)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->gflownet==0.1.0) (9.4.0)\n",
            "Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from tables->gflownet==0.1.0) (3.0.4)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from tables->gflownet==0.1.0) (2.8.7)\n",
            "Requirement already satisfied: blosc2~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from tables->gflownet==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tables->gflownet==0.1.0) (23.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from tables->gflownet==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (3.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->gflownet==0.1.0) (3.0.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from blosc2~=2.0.0->tables->gflownet==0.1.0) (1.0.7)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->gflownet==0.1.0)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->gflownet==0.1.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->gflownet==0.1.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->gflownet==0.1.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->gflownet==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->gflownet==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->gflownet==0.1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->gflownet==0.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->gflownet==0.1.0) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch->gflownet==0.1.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch->gflownet==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch->gflownet==0.1.0) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch->gflownet==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch->gflownet==0.1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch->gflownet==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->gflownet==0.1.0) (2.1.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch->gflownet==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch->gflownet==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->gflownet==0.1.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->gflownet==0.1.0) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->botorch->gflownet==0.1.0) (1.3.0)\n",
            "Building wheels for collected packages: gflownet, antlr4-python3-runtime\n",
            "  Building editable for gflownet (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gflownet: filename=gflownet-0.1.0-0.editable-py2.py3-none-any.whl size=4586 sha256=570189b689c71ac9fe9fefdb22fcc13109fdc08aa2874e956261f9bbfe54a696\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iqc6goin/wheels/e5/41/33/462d77294be6403996cc740cd3c5d41dfdcbb15d266ec07cd3\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=9643b2fb8c1eb4a13a42f6c306bb95d9e2ccbafbbb6dd24ecb03513fd758d565\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built gflownet antlr4-python3-runtime\n",
            "Installing collected packages: pyro-api, antlr4-python3-runtime, typeguard, smmap, rdkit, omegaconf, jaxtyping, gitdb, pyro-ppl, linear-operator, gitpython, gpytorch, botorch, gflownet\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 botorch-0.9.3 gflownet-0.1.0 gitdb-4.0.11 gitpython-3.1.40 gpytorch-1.11 jaxtyping-0.2.23 linear-operator-0.5.1 omegaconf-2.3.0 pyro-api-0.1.2 pyro-ppl-1.8.6 rdkit-2023.9.1 smmap-5.0.1 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fe8U7BoN2u5E",
        "outputId": "1d27e653-dfc0-41e3-ce8f-99ba8dc06cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import socket\n",
        "from typing import Callable, Dict, List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_geometric.data as gd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.rdchem import Mol as RDMol\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from gflownet.config import Config\n",
        "from gflownet.envs.frag_mol_env import FragMolBuildingEnvContext, Graph\n",
        "from gflownet.models import bengio2021flow\n",
        "from gflownet.online_trainer import StandardOnlineTrainer\n",
        "from gflownet.trainer import FlatRewards, GFNTask, RewardScalar\n",
        "from gflownet.utils.conditioning import TemperatureConditional\n"
      ],
      "metadata": {
        "id": "hLYKO3802zHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SEHTask(GFNTask):\n",
        "    \"\"\"Sets up a task where the reward is computed using a proxy for the binding energy of a molecule to\n",
        "    Soluble Epoxide Hydrolases.\n",
        "\n",
        "    The proxy is pretrained, and obtained from the original GFlowNet paper, see `gflownet.models.bengio2021flow`.\n",
        "\n",
        "    This setup essentially reproduces the results of the Trajectory Balance paper when using the TB\n",
        "    objective, or of the original paper when using Flow Matching.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        cfg: Config,\n",
        "        rng: np.random.Generator = None,\n",
        "        wrap_model: Callable[[nn.Module], nn.Module] = None,\n",
        "    ):\n",
        "        self._wrap_model = wrap_model\n",
        "        self.rng = rng\n",
        "        self.models = self._load_task_models()\n",
        "        self.dataset = dataset\n",
        "        self.temperature_conditional = TemperatureConditional(cfg, rng)\n",
        "        self.num_cond_dim = self.temperature_conditional.encoding_size()\n",
        "\n",
        "    def flat_reward_transform(self, y: Union[float, Tensor]) -> FlatRewards:\n",
        "        return FlatRewards(torch.as_tensor(y) / 8)\n",
        "\n",
        "    def inverse_flat_reward_transform(self, rp):\n",
        "        return rp * 8\n",
        "\n",
        "    def _load_task_models(self):\n",
        "        model = bengio2021flow.load_original_model()\n",
        "        model, self.device = self._wrap_model(model, send_to_device=True)\n",
        "        return {\"seh\": model}\n",
        "\n",
        "    def sample_conditional_information(self, n: int, train_it: int) -> Dict[str, Tensor]:\n",
        "        return self.temperature_conditional.sample(n)\n",
        "\n",
        "    def cond_info_to_logreward(self, cond_info: Dict[str, Tensor], flat_reward: FlatRewards) -> RewardScalar:\n",
        "        return RewardScalar(self.temperature_conditional.transform(cond_info, flat_reward))\n",
        "\n",
        "    def compute_flat_rewards(self, mols: List[RDMol]) -> Tuple[FlatRewards, Tensor]:\n",
        "        graphs = [bengio2021flow.mol2graph(i) for i in mols]\n",
        "        is_valid = torch.tensor([i is not None for i in graphs]).bool()\n",
        "        if not is_valid.any():\n",
        "            return FlatRewards(torch.zeros((0, 1))), is_valid\n",
        "        batch = gd.Batch.from_data_list([i for i in graphs if i is not None])\n",
        "        batch.to(self.device)\n",
        "        preds = self.models[\"seh\"](batch).reshape((-1,)).data.cpu()\n",
        "        preds[preds.isnan()] = 0\n",
        "        preds = self.flat_reward_transform(preds).clip(1e-4, 100).reshape((-1, 1))\n",
        "        return FlatRewards(preds), is_valid\n",
        "\n",
        "\n",
        "SOME_MOLS = [\n",
        "    \"O=C(NCc1cc(CCc2cccc(N3CCC(c4cc(-c5cc(-c6cncnc6)[nH]n5)ccn4)CC3)c2)ccn1)c1cccc2ccccc12\",\n",
        "    \"O=c1nc2[nH]c3cc(-c4cc(C5CC(c6ccc(CNC7CCOC7c7csc(C8=CC(c9ccc%10ccccc%10c9)CCC8)n7)cc6)CO5)c[nH]4)ccc3nc-2c(=O)[nH]1\",\n",
        "    \"c1ccc(-c2cnn(-c3cc(-c4cc(CCc5cc(C6CCC(c7cc(-c8ccccc8)[nH]n7)CO6)ncn5)n[nH]4)ccn3)c2)cc1\",\n",
        "    \"O=C(NCc1cc(C2CCNC2C2CCNC2)ncn1)c1cccc(-c2cccc(-c3cccc(C4CCC(c5ccccc5)CO4)c3)c2)c1\",\n",
        "    \"O=C(NCc1cccc(C2COC(c3ccc4nc5c(=O)[nH]c(=O)nc-5[nH]c4c3)C2)c1)c1cccc(CCc2cccc(-c3ncnc4c3ncn4C3CCCN3)c2)c1\",\n",
        "    \"O=C(NCc1ccc(OCc2ccc(-c3ccncc3C3CCNCC3)cn2)cc1)c1cccc(N2CCC(C3CCCN3)CC2)n1\",\n",
        "    \"O=C(NCc1ccc(C2CCC(c3cccc(-c4cccc(C5CCOC5)c4)c3)CO2)cn1)c1ccc(-n2ccc(-c3ccc4nc5c(=O)[nH]c(=O)nc-5[nH]c4c3)n2)cn1\",\n",
        "    \"O=C(NCc1nc2c(c(=O)[nH]1)NC(c1cn(N3CCN(c4ccc5nc6c(=O)[nH]c(=O)nc-6[nH]c5c4)CC3)c(=O)[nH]c1=O)CN2)c1ccc[n+](-c2cccc(-c3nccc(-c4ccccc4)n3)c2)c1\",\n",
        "    \"C1=C(C2CCC(c3ccnc(-c4ccc(CNC5CCC(c6ccncc6)OC5)cc4)n3)CO2)CCC(c2cc(-c3cncnc3)c3ccccc3c2)C1\",\n",
        "    \"O=C(NCc1cccc(-c2nccc(-c3cc(-c4ccc5ccccc5c4)n[nH]3)n2)c1)C1CCC(C2CCC(c3cn(-c4ccc5nc6c(=O)[nH]c(=O)nc-6[nH]c5c4)c(=O)[nH]c3=O)OC2)O1\",\n",
        "    \"O=C(Nc1ccc2ccccc2c1)c1cccc(-c2cccc(CNN3CCN(C4CCCC(c5cccc(C6CCCN6)c5)C4)CC3)c2)c1\",\n",
        "    \"O=C(NCC1CC=C(c2cc(CCc3c[nH]c(-c4cccc(-c5ccccc5)c4)c3)n[nH]2)CC1)c1cccc(C2CCNC2)n1\",\n",
        "    \"O=C(Nc1nccc(CNc2cc(C3CCNC3)n[nH]2)n1)c1nccc(C2CCC(C3CCNCC3c3ccc4ccccc4c3)CO2)n1\",\n",
        "    \"C1=C(C2CCC(c3ccc(-c4cc(C5CCCNC5)n[nH]4)cc3)OC2)CCCC1CCc1cccc(-c2cccc(-c3ncnc4[nH]cnc34)c2)n1\",\n",
        "    \"O=C(NCc1cc(C2CCC(C3CCN(c4cc(-c5nccc(-c6cccc(-c7ccccc7)c6)n5)c[nH]4)CC3)CO2)ccn1)c1ccccc1\",\n",
        "    \"O=C(NCc1cccc(-c2ccn(NCc3ccc(-c4cc(C5CNC(c6ccncc6)C5)c[nH]4)cc3)n2)c1)c1ccc2ccccc2c1\",\n",
        "    \"O=c1nc2n(-c3cccc(OCc4cccc(CNC5CCC(c6cccc(-c7ccc(C8CCNC8)cc7)c6)OC5)c4)n3)c3ccccc3nc-2c(=O)[nH]1\",\n",
        "    \"O=C(NCc1ccc(C2OCCC2C2CC(c3ccnc(-c4ccc5ccccc5c4)c3)CO2)cc1)c1nccc(N2C=CCC(c3ccccc3)=C2)n1\",\n",
        "    \"O=C(NCNC(=O)c1cccc(C(=O)NCc2cccc(-c3ccc4[nH]c5nc(=O)[nH]c(=O)c-5nc4c3)c2)n1)c1ccnc(-c2nccc(C3CCCN3)n2)c1\",\n",
        "    \"O=c1nc2[nH]c3cc(C4CCC(c5ccc(-c6cc(C7CCC(C8CCCC(C9CCC(c%10ccc(-c%11cncnc%11)cc%10)O9)O8)OC7)ccn6)cn5)CO4)ccc3nc-2c(=O)[nH]1\",\n",
        "    \"O=c1[nH]c(CNc2cc(-c3cccc(-n4ccc(-c5ccc6ccccc6c5)n4)c3)c[nH]2)nc2c1NC(n1ccc(C3CCC(c4cccnc4)CO3)n1)CN2\",\n",
        "    \"O=c1nc2[nH]c3cc(C=CC4COC(C5CCCC(C6CCOC(C7CCC(c8cccc(-c9ccnc(-c%10ccc%11ccccc%11c%10)n9)c8)CO7)C6)O5)C4)ccc3nc-2c(=O)[nH]1\",\n",
        "    \"c1ccc2c(C3CC(CNc4ccnc(C5CCNC5)c4)CO3)cc(NCc3ccc(-c4cc(C5CCNC5)c[nH]4)cc3)cc2c1\",\n",
        "    \"O=C(NCc1nccc(C2CC(C(=O)NC3CCC(c4ccc5nc6c(=O)[nH]c(=O)nc-6[nH]c5c4)CO3)CCO2)n1)c1ccnc(-n2cc(-n3cnc4cncnc43)cn2)n1\",\n",
        "    \"O=C(NCc1ccc(-c2ccccc2)cc1)c1cccc(C(=O)NCc2nccc(N3C=CCC(c4ncnc5c4ncn5-c4cccc5ccccc45)=C3)n2)c1\",\n",
        "]\n",
        "\n",
        "\n",
        "class LittleSEHDataset(Dataset):\n",
        "    \"\"\"Note: this dataset isn't used by default, but turning it on showcases some features of this codebase.\n",
        "\n",
        "    To turn on, self `cfg.algo.offline_ratio > 0`\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.props: List[Tensor] = []\n",
        "        self.mols: List[Graph] = []\n",
        "\n",
        "    def setup(self, task, ctx):\n",
        "        rdmols = [Chem.MolFromSmiles(i) for i in SOME_MOLS]\n",
        "        self.mols = [ctx.mol_to_graph(i) for i in rdmols]\n",
        "        self.props = task.compute_flat_rewards(rdmols)[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mols)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.mols[index], self.props[index]\n",
        "\n",
        "\n",
        "class SEHFragTrainer(StandardOnlineTrainer):\n",
        "    task: SEHTask\n",
        "    training_data: LittleSEHDataset\n",
        "\n",
        "    def set_default_hps(self, cfg: Config):\n",
        "        cfg.hostname = socket.gethostname()\n",
        "        cfg.pickle_mp_messages = False\n",
        "        cfg.num_workers = 8\n",
        "        cfg.opt.learning_rate = 1e-4\n",
        "        cfg.opt.weight_decay = 1e-8\n",
        "        cfg.opt.momentum = 0.9\n",
        "        cfg.opt.adam_eps = 1e-8\n",
        "        cfg.opt.lr_decay = 20_000\n",
        "        cfg.opt.clip_grad_type = \"norm\"\n",
        "        cfg.opt.clip_grad_param = 10\n",
        "        cfg.algo.global_batch_size = 64\n",
        "        cfg.algo.offline_ratio = 0\n",
        "        cfg.model.num_emb = 128\n",
        "        cfg.model.num_layers = 4\n",
        "\n",
        "        cfg.algo.method = \"TB\"\n",
        "        cfg.algo.max_nodes = 9\n",
        "        cfg.algo.sampling_tau = 0.9\n",
        "        cfg.algo.illegal_action_logreward = -75\n",
        "        cfg.algo.train_random_action_prob = 0.0\n",
        "        cfg.algo.valid_random_action_prob = 0.0\n",
        "        cfg.algo.valid_offline_ratio = 0\n",
        "        cfg.algo.tb.epsilon = None\n",
        "        cfg.algo.tb.bootstrap_own_reward = False\n",
        "        cfg.algo.tb.Z_learning_rate = 1e-3\n",
        "        cfg.algo.tb.Z_lr_decay = 50_000\n",
        "        cfg.algo.tb.do_parameterize_p_b = False\n",
        "        cfg.algo.tb.do_sample_p_b = True\n",
        "\n",
        "        cfg.replay.use = False\n",
        "        cfg.replay.capacity = 10_000\n",
        "        cfg.replay.warmup = 1_000\n",
        "\n",
        "    def setup_task(self):\n",
        "        self.task = SEHTask(\n",
        "            dataset=self.training_data,\n",
        "            cfg=self.cfg,\n",
        "            rng=self.rng,\n",
        "            wrap_model=self._wrap_for_mp,\n",
        "        )\n",
        "\n",
        "    def setup_data(self):\n",
        "        super().setup_data()\n",
        "        self.training_data = LittleSEHDataset()\n",
        "\n",
        "    def setup_env_context(self):\n",
        "        self.ctx = FragMolBuildingEnvContext(max_frags=self.cfg.algo.max_nodes, num_cond_dim=self.task.num_cond_dim)\n",
        "\n",
        "    def setup(self):\n",
        "        super().setup()\n",
        "        self.training_data.setup(self.task, self.ctx)\n"
      ],
      "metadata": {
        "id": "G-b280tU8zrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hps = {\n",
        "    \"log_dir\": \"./logs/debug_run_seh_frag_pb\",\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"overwrite_existing_exp\": True,\n",
        "    \"num_training_steps\": 10,\n",
        "    \"num_workers\": 1,\n",
        "    \"opt\": {\n",
        "        \"lr_decay\": 20000,\n",
        "    },\n",
        "    \"algo\": {\"sampling_tau\": 0.99, \"offline_ratio\": 0.0},\n",
        "    \"cond\": {\n",
        "        \"temperature\": {\n",
        "            \"sample_dist\": \"uniform\",\n",
        "            \"dist_params\": [0, 64.0],\n",
        "        }\n",
        "    },\n",
        "}\n",
        "if os.path.exists(hps[\"log_dir\"]):\n",
        "    if hps[\"overwrite_existing_exp\"]:\n",
        "        shutil.rmtree(hps[\"log_dir\"])\n",
        "    else:\n",
        "        raise ValueError(f\"Log dir {hps['log_dir']} already exists. Set overwrite_existing_exp=True to delete it.\")\n",
        "os.makedirs(hps[\"log_dir\"])\n",
        "\n",
        "trial = SEHFragTrainer(hps)\n",
        "trial.print_every = 1\n",
        "trial.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xnrp7ybg84zo",
        "outputId": "83e217cd-79d1-4d40-9e6e-489a14f59483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Hyperparameters:\n",
            "\n",
            "log_dir: ./logs/debug_run_seh_frag_pb\n",
            "device: cpu\n",
            "seed: 0\n",
            "validate_every: 1000\n",
            "checkpoint_every: null\n",
            "print_every: 100\n",
            "start_at_step: 0\n",
            "num_final_gen_steps: null\n",
            "num_training_steps: 10\n",
            "num_workers: 1\n",
            "hostname: d0b0c53fb80b\n",
            "pickle_mp_messages: false\n",
            "git_hash: d3ddff3\n",
            "overwrite_existing_exp: true\n",
            "algo:\n",
            "  method: TB\n",
            "  global_batch_size: 64\n",
            "  max_len: 128\n",
            "  max_nodes: 9\n",
            "  max_edges: 128\n",
            "  illegal_action_logreward: -75.0\n",
            "  offline_ratio: 0.0\n",
            "  valid_offline_ratio: 0.0\n",
            "  train_random_action_prob: 0.0\n",
            "  valid_random_action_prob: 0.0\n",
            "  valid_sample_cond_info: true\n",
            "  sampling_tau: 0.99\n",
            "  tb:\n",
            "    bootstrap_own_reward: false\n",
            "    epsilon: null\n",
            "    reward_loss_multiplier: 1.0\n",
            "    variant: TB\n",
            "    do_correct_idempotent: false\n",
            "    do_parameterize_p_b: false\n",
            "    do_sample_p_b: true\n",
            "    do_length_normalize: false\n",
            "    subtb_max_len: 128\n",
            "    Z_learning_rate: 0.001\n",
            "    Z_lr_decay: 50000.0\n",
            "    cum_subtb: true\n",
            "  moql:\n",
            "    gamma: 1.0\n",
            "    num_omega_samples: 32\n",
            "    num_objectives: 2\n",
            "    lambda_decay: 10000\n",
            "    penalty: -10.0\n",
            "  a2c:\n",
            "    entropy: 0.01\n",
            "    gamma: 1.0\n",
            "    penalty: -10.0\n",
            "  fm:\n",
            "    epsilon: 1.0e-38\n",
            "    balanced_loss: false\n",
            "    leaf_coef: 10.0\n",
            "    correct_idempotent: false\n",
            "  sql:\n",
            "    alpha: 0.01\n",
            "    gamma: 1.0\n",
            "    penalty: -10.0\n",
            "model:\n",
            "  num_layers: 4\n",
            "  num_emb: 128\n",
            "  dropout: 0.0\n",
            "  graph_transformer:\n",
            "    num_heads: 2\n",
            "    ln_type: pre\n",
            "    num_mlp_layers: 0\n",
            "  seq_transformer:\n",
            "    num_heads: 2\n",
            "    posenc: Rotary\n",
            "opt:\n",
            "  opt: adam\n",
            "  learning_rate: 0.0001\n",
            "  lr_decay: 20000.0\n",
            "  weight_decay: 1.0e-08\n",
            "  momentum: 0.9\n",
            "  clip_grad_type: norm\n",
            "  clip_grad_param: 10.0\n",
            "  adam_eps: 1.0e-08\n",
            "replay:\n",
            "  use: false\n",
            "  capacity: 10000\n",
            "  warmup: 1000\n",
            "  hindsight_ratio: 0.0\n",
            "task:\n",
            "  qm9:\n",
            "    h5_path: ./data/qm9/qm9.h5\n",
            "    model_path: ./data/qm9/qm9_model.pt\n",
            "  seh: {}\n",
            "  seh_moo:\n",
            "    use_steer_thermometer: false\n",
            "    preference_type: dirichlet\n",
            "    focus_type: null\n",
            "    focus_dirs_listed: null\n",
            "    focus_cosim: 0.0\n",
            "    focus_limit_coef: 1.0\n",
            "    focus_model_training_limits: null\n",
            "    focus_model_state_space_res: null\n",
            "    max_train_it: null\n",
            "    n_valid: 15\n",
            "    n_valid_repeats: 128\n",
            "    objectives:\n",
            "    - seh\n",
            "    - qed\n",
            "    - sa\n",
            "    - mw\n",
            "cond:\n",
            "  temperature:\n",
            "    sample_dist: uniform\n",
            "    dist_params:\n",
            "    - 0\n",
            "    - 64.0\n",
            "    num_thermometer_dim: 32\n",
            "  moo:\n",
            "    num_objectives: 2\n",
            "    num_thermometer_dim: 16\n",
            "  weighted_prefs:\n",
            "    preference_type: dirichlet\n",
            "  focus_region:\n",
            "    focus_type: learned-tabular\n",
            "    use_steer_thermomether: false\n",
            "    focus_cosim: 0.98\n",
            "    focus_limit_coef: 0.1\n",
            "    focus_model_training_limits:\n",
            "    - 0.25\n",
            "    - 0.75\n",
            "    focus_model_state_space_res: 30\n",
            "    max_train_it: 20000\n",
            "\n",
            "05/11/2023 18:24:00 - INFO - logger - Starting training\n",
            "05/11/2023 18:24:00 - INFO - logger - Starting training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:logger:Starting training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/11/2023 18:24:10 - INFO - logger - iteration 1 : offline_loss:0.00 online_loss:1898.15 reward_loss:0.00 invalid_trajectories:0.00 invalid_logprob:0.00 invalid_losses:0.00 logZ:0.15 loss:1898.15\n",
            "05/11/2023 18:24:10 - INFO - logger - iteration 1 : offline_loss:0.00 online_loss:1898.15 reward_loss:0.00 invalid_trajectories:0.00 invalid_logprob:0.00 invalid_losses:0.00 logZ:0.15 loss:1898.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:logger:iteration 1 : offline_loss:0.00 online_loss:1898.15 reward_loss:0.00 invalid_trajectories:0.00 invalid_logprob:0.00 invalid_losses:0.00 logZ:0.15 loss:1898.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/11/2023 18:24:15 - INFO - logger - iteration 2 : offline_loss:0.00 online_loss:1665.16 reward_loss:0.00 invalid_trajectories:0.00 invalid_logprob:0.00 invalid_losses:0.00 logZ:-0.08 loss:1665.16\n",
            "05/11/2023 18:24:15 - INFO - logger - iteration 2 : offline_loss:0.00 online_loss:1665.16 reward_loss:0.00 invalid_trajectories:0.00 invalid_logprob:0.00 invalid_losses:0.00 logZ:-0.08 loss:1665.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:logger:iteration 2 : offline_loss:0.00 online_loss:1665.16 reward_loss:0.00 invalid_trajectories:0.00 invalid_logprob:0.00 invalid_losses:0.00 logZ:-0.08 loss:1665.16\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-65cb89f6952e>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSEHFragTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/gflownet/src/gflownet/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, logger)\u001b[0m\n\u001b[1;32m    325\u001b[0m                 )\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gflownet/src/gflownet/trainer.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, batch, epoch_idx, batch_idx, train_it)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss is not finite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mstep_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_parameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parameters are not finite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gflownet/src/gflownet/online_trainer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1rYRfcTARt9",
        "outputId": "95aa0cfa-531e-409a-c92a-18eb1f3a59be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.SEHFragTrainer at 0x7d887702b340>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qdHnSUL2LV8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}